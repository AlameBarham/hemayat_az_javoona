{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "623899e2",
      "metadata": {},
      "source": [
        "**note that if you run the next cell, you will download 170M!!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32fc6352",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32fc6352",
        "outputId": "be31a6a3-74ef-47b1-9386-e12b848ca9b7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset_train = CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "dataset_test = CIFAR10(\"./\", train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "-wiCFJ5i543z",
      "metadata": {
        "id": "-wiCFJ5i543z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h0jci7AbR7am",
      "metadata": {
        "id": "h0jci7AbR7am"
      },
      "source": [
        "# with Linear Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5e057e7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dTCvBGde6rap",
      "metadata": {
        "id": "dTCvBGde6rap"
      },
      "outputs": [],
      "source": [
        "# class Network(nn.Module):\n",
        "#   def __init__(self, n_input=3 * 32 * 32):\n",
        "#     super().__init__()\n",
        "#     self.fc1 = nn.Linear(n_input, 128)\n",
        "#     self.fc2 = nn.Linear(128, 64)\n",
        "#     self.fc3 = nn.Linear(64, 32)\n",
        "#     self.fc4 = nn.Linear(32, 10)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#                                   # x.dim -> 128 * 3 * 32 * 32\n",
        "#     x = x.reshape(x.shape[0], -1) # x.shape[0] = 128\n",
        "#                                   # x.dim -> 128 * (3 * 32 * 32)\n",
        "#     x = torch.relu(self.fc1(x))\n",
        "#     x = torch.relu(self.fc2(x))\n",
        "#     x = torch.relu(self.fc3(x))\n",
        "#     x = self.fc4(x)\n",
        "#     return x\n",
        "\n",
        "# # model initializing\n",
        "# model = Network().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "btxc_AWE_1WB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btxc_AWE_1WB",
        "outputId": "1396c11d-8595-439e-e4a4-97bddbbe9a64"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import f1_score\n",
        "# import numpy as np\n",
        "\n",
        "# print(\"metric is f1_score\\n\\n\", \"-\"*20)\n",
        "# for epochs in range(10):\n",
        "#   model.train()\n",
        "#   total_loss = 0\n",
        "\n",
        "#   # Training NN's weights\n",
        "#   for X_batch, y_batch in train_loader:\n",
        "#     X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "#     optimizer.zero_grad()\n",
        "#     y_pred = model(X_batch)\n",
        "#     loss = criterion(y_pred, y_batch)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     total_loss += loss.item()\n",
        "\n",
        "#     # Evaluating NN's weights\n",
        "#   with torch.no_grad():\n",
        "#     model.eval()\n",
        "#     train_f1, test_f1 = [], []\n",
        "\n",
        "#       # Train\n",
        "#     for X_batch, y_batch in train_loader:\n",
        "#       X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "#       y_pred = model(X_batch).cpu().numpy().argmax(axis=1)\n",
        "#       train_f1.append(f1_score(y_pred, y_batch.cpu().numpy(), average=\"macro\"))\n",
        "#     train_f1_mean = np.mean(train_f1)\n",
        "\n",
        "#       # Test\n",
        "#     for X_batch, y_batch in test_loader:\n",
        "#       X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "#       y_pred = model(X_batch).cpu().numpy().argmax(axis=1)\n",
        "#       test_f1.append(f1_score(y_pred, y_batch.cpu().numpy(), average=\"macro\"))\n",
        "#     test_f1_mean = np.mean(test_f1)\n",
        "\n",
        "#     # printing the results of each epochs:\n",
        "#     print(f\"epoch: {epochs+1}, loss = {total_loss / 32:.4f}, train: {np.round(train_f1_mean*100, 4)}%, test: {np.round(test_f1_mean*100, 4)} %\",\"\\n\", \"-\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kkgGXtu4R_Vy",
      "metadata": {
        "id": "kkgGXtu4R_Vy"
      },
      "source": [
        "# with convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l9vVybycSCU8",
      "metadata": {
        "id": "l9vVybycSCU8"
      },
      "outputs": [],
      "source": [
        "class ConvolutionNetwork(nn.Module):\n",
        "  def __init__(self, n_input=3 * 32 * 32):\n",
        "    super().__init__()\n",
        "\n",
        "    # input shape is 128 * 3 * 32 * 32\n",
        "    self.conv1 = nn.Conv2d(3, 32, (3, 3), (2, 2))   # shape conv1 : 128, 32, 15, 15\n",
        "    self.conv2 = nn.Conv2d(32, 64, (3, 3), (2, 2))  # shape conv2 : 128, 64, 7, 7\n",
        "    self.conv3 = nn.Conv2d(64, 256, (3, 3), (2, 2)) # shape conv3 : 128, 256, 3, 3\n",
        "\n",
        "    self.linear1 = nn.Linear(2304, 128)\n",
        "    self.linear2 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Convolutions\n",
        "    x = torch.relu(self.conv1(x))\n",
        "    x = torch.relu(self.conv2(x))\n",
        "    x = torch.relu(self.conv3(x))\n",
        "                                  # X.dim => 128(n_samples) * 256 * 3 * 3\n",
        "    x = x.reshape(x.shape[0], -1) # X.shape[0] is the n_samples\n",
        "                                  # x.dim => 128 * 2304\n",
        "                                  # we made 1152 features of image by conv\n",
        "    # Linears\n",
        "    x = torch.relu(self.linear1(x))\n",
        "    x = torch.relu(self.linear2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "# model initializing\n",
        "model2 = ConvolutionNetwork().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(params=model2.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "444e0eac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(1, 64, 7, 7).to(device)\n",
        "print(model2.conv3(x).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "O72E9z-KSF_S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "O72E9z-KSF_S",
        "outputId": "44ddb93e-5905-47c4-8c71-8f364387dc5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metric is f1_score\n",
            "\n",
            " --------------------\n",
            "epoch: 1, loss = 3.9620, train: 54.2238%, test: 51.8795 % \n",
            " --------------------\n",
            "epoch: 2, loss = 3.9153, train: 50.8523%, test: 48.275 % \n",
            " --------------------\n",
            "epoch: 3, loss = 3.8553, train: 55.077%, test: 51.1424 % \n",
            " --------------------\n",
            "epoch: 4, loss = 3.8106, train: 55.0521%, test: 51.5891 % \n",
            " --------------------\n",
            "epoch: 5, loss = 3.7483, train: 56.7275%, test: 52.9934 % \n",
            " --------------------\n",
            "epoch: 6, loss = 3.7043, train: 55.3086%, test: 51.9396 % \n",
            " --------------------\n",
            "epoch: 7, loss = 3.6609, train: 57.0707%, test: 52.4857 % \n",
            " --------------------\n",
            "epoch: 8, loss = 3.6054, train: 58.1553%, test: 53.3237 % \n",
            " --------------------\n",
            "epoch: 9, loss = 3.5645, train: 57.9968%, test: 53.1006 % \n",
            " --------------------\n",
            "epoch: 10, loss = 3.5136, train: 59.3501%, test: 53.7136 % \n",
            " --------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"metric is f1_score\\n\\n\", \"-\"*20)\n",
        "for epochs in range(10):\n",
        "  model2.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  # Training NN's weights\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "    opt.zero_grad()\n",
        "    y_pred = model2(X_batch)\n",
        "    loss = criterion(y_pred, y_batch)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Evaluating NN's weights\n",
        "  with torch.no_grad():\n",
        "    model2.eval()\n",
        "    train_f1, test_f1 = [], []\n",
        "\n",
        "      # Train\n",
        "    for X_batch, y_batch in train_loader:\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      y_pred = model2(X_batch).argmax(axis=1).cpu().numpy()\n",
        "      train_f1.append(f1_score(y_pred, y_batch.cpu().numpy(), average=\"macro\"))\n",
        "    train_f1_mean = np.mean(train_f1)\n",
        "\n",
        "      # Test\n",
        "    for X_batch, y_batch in test_loader:\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      y_pred = model2(X_batch).argmax(axis=1).cpu().numpy()\n",
        "      test_f1.append(f1_score(y_pred, y_batch.cpu().numpy(), average=\"macro\"))\n",
        "    test_f1_mean = np.mean(test_f1)\n",
        "\n",
        "    # printing the results of each epochs:\n",
        "    print(f\"epoch: {epochs+1}, loss = {total_loss / 128:.4f}, train: {np.round(train_f1_mean*100, 4)}%, test: {np.round(test_f1_mean*100, 4)} %\",\"\\n\", \"-\"*20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
